{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Electricity Spot Price predictions for SA, NSW, VIC and QLD with data from amphora.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import floor\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from joblib import dump, load\n",
    "from modules.modules import APIfetch\n",
    "import modules.modules.tool as tool\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from modules.modules import func_CNN_1, func_CNN_2, func_NN_3, func_CNN_1_inverted\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data, creating differential features, creating X, and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'id': '',\n",
    "        'start_time': datetime(2019,11,9,11),#YYYY,MM,DD,hh,mm\n",
    "        'end_time': datetime.now(),\n",
    "        #'filter': ''\n",
    "        }\n",
    "\n",
    "e_ids = {'electricity_NSW': 'ecc5263e-83b6-42d6-8852-64beffdf204e',  \n",
    "        'electricity_QLD': 'ef22fa0f-010c-4ab1-8a28-a8963f838ce9',  \n",
    "        'electricity_VIC': '3b66da5a-0723-4778-98fc-02d619c70664', \n",
    "        'electricity_SA': '89c2e30d-78c8-46ef-b591-140edd84ddb6'}  \n",
    "\n",
    "electricity_dict = {}\n",
    "for _ in e_ids:\n",
    "    params['id'] = e_ids[_]\n",
    "    data_tempe = APIfetch.fetch_data(params)\n",
    "    # to prevent the API from returning only a subset of the desired\n",
    "    # data, this check ensures that the returned data is <max\n",
    "    assert data_tempe.shape[0]<=10000, 'API maximum row number surpassed'\n",
    "    if 'filter' in params:\n",
    "        del data_tempe['periodType.{}'.format(params['filter'])]\n",
    "    else:\n",
    "        del data_tempe['periodType']\n",
    "\n",
    "    # add the _state suffix to column names:\n",
    "    data_tempe.columns = [col +'_'+_[_.index('_')+1:] for col in data_tempe.columns]\n",
    "    data_tempe = data_tempe.sort_index()\n",
    "\n",
    "    # prevent some weird pandas 'feature' from resampling the df:\n",
    "    data_tempe = data_tempe.apply(pd.to_numeric, errors='coerce')\n",
    "    electricity_dict[_] = data_tempe.resample('60T').mean()\n",
    "\n",
    "forecast_dict = {}\n",
    "for _ in e_ids:\n",
    "    params['filter'] = 'Forecast'\n",
    "    params['id'] = e_ids[_]\n",
    "    data_tempf = APIfetch.fetch_data(params)\n",
    "    del data_tempf['periodType.Forecast']\n",
    "\n",
    "    # add the _state suffix to column names:\n",
    "    data_tempf.columns = [col +'_'+_[_.index('_')+1:] for col in data_tempf.columns]\n",
    "    data_tempf = data_tempf.sort_index()\n",
    "\n",
    "    # prevent some weird pandas 'feature' from resampling the df:\n",
    "    data_tempf = data_tempf.apply(pd.to_numeric,                       errors='coerce')\n",
    "    forecast_dict[_] = data_tempf.resample('60T').mean()\n",
    "    del params['filter']\n",
    "    \n",
    "w_ids = {'weather_NSW': '11fd3d6a-12e4-4767-9d52-03271b543c66',  \n",
    "        'weather_QLD': 'a46f461f-f7ee-4cc5-a1e4-569960ea5ed8',  \n",
    "        'weather_VIC': 'd48ac35f-c658-41c1-909a-f662d6f3a972', \n",
    "        'weather_SA': 'f860ba45-9dda-41e0-91aa-73901a323318'}  \n",
    "\n",
    "weather_dict = {}\n",
    "for _ in w_ids:\n",
    "    params['id'] = w_ids[_]\n",
    "    data_tempw = APIfetch.fetch_data(params)\n",
    "\n",
    "    if 'filter' in params:\n",
    "        del data_tempw['description.{}'.format(params['filter'])]\n",
    "    else:\n",
    "        del data_tempw['description']\n",
    "\n",
    "    # add the _state suffix to column names:\n",
    "    data_tempw.columns = [col +'_'+_[_.index('_')+1:] for col in data_tempw.columns]\n",
    "    data_tempw = data_tempw.sort_index()\n",
    "\n",
    "    # prevent some weird pandas 'feature' from resampling the df:\n",
    "    data_tempw = data_tempw.apply(pd.to_numeric,                       errors='coerce')\n",
    "    weather_dict[_] = data_tempw.resample('60T').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all electricity data\n",
    "df_elec = electricity_dict['electricity_NSW'].join(electricity_dict['electricity_QLD'])\n",
    "df_elec = df_elec.join(electricity_dict['electricity_VIC'])\n",
    "df_elec = df_elec.join(electricity_dict['electricity_SA'])\n",
    "df_elec = tool.create_diffs(df_elec, list(range(len(df_elec.columns))))\n",
    "\n",
    "# join all fore data\n",
    "df_fore = forecast_dict['electricity_NSW'].join(forecast_dict['electricity_QLD'])\n",
    "df_fore = df_fore.join(forecast_dict['electricity_VIC'])\n",
    "df_fore = df_fore.join(forecast_dict['electricity_SA'])\n",
    "df_fore = tool.create_diffs(df_fore, list(range(len(df_fore.columns))))\n",
    "\n",
    "# outer join for all weather data\n",
    "df_weather = weather_dict['weather_NSW'].join(weather_dict['weather_QLD'])\n",
    "df_weather = df_weather.join(weather_dict['weather_VIC'])\n",
    "df_weather = df_weather.join(weather_dict['weather_SA'])\n",
    "df_weather = tool.create_diffs(df_weather, list(range(len(df_weather.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-sort dataset columns and extract test data y\n",
    "df_elec = df_elec.reindex(sorted(df_elec.columns), axis=1)\n",
    "y = df_elec.iloc[:,8:12]\n",
    "for _ in y.columns:\n",
    "    del df_elec[_]\n",
    "\n",
    "print('elec shape: ',df_elec.shape)\n",
    "print('y shape: ',y.shape)\n",
    "\n",
    "# Create complete dataset of all train variables\n",
    "df_all = df_elec.join(df_weather)\n",
    "df_all = df_all.join(df_fore)\n",
    "\n",
    "print('df_all shape: ',df_all.shape)\n",
    "\n",
    "#also atomise the datetime index, will be v useful for large-ish datasets, as spikes on e.g. weekends, become identifiable for the model:\n",
    "# split dates into year month day day of week hour, etc for additional features \n",
    "df_all = tool.split_dates_df(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to save and/or stitch previously downloaded data csv's:\n",
    "# df_all.to_csv('./2_raw_data/df_all-to-7thJan1.csv',\n",
    "#               header=True,\n",
    "#               index=True)\n",
    "# y.to_csv('./2_raw_data/y-to-7thJan1.csv',\n",
    "#          header=True,\n",
    "#          index=True)\n",
    "\n",
    "# and the latter bit:\n",
    "df_all = pd.read_csv('./2_raw_data/df_all-to-7thJan.csv', index_col=0)\n",
    "df_all = df_all.append(pd.read_csv('./2_raw_data/df_all-to-3rdDec.csv',\n",
    "                          index_col=0))\n",
    "df_all = df_all.append(pd.read_csv('./2_raw_data/df_all-to-19thDec.csv',\n",
    "                          index_col=0))                          \n",
    "df_all = df_all.sort_index()\n",
    "df_all.index = pd.to_datetime(df_all.index)\n",
    "y = pd.read_csv('./2_raw_data/y-to-7thJan.csv', index_col=0)\n",
    "y = y.append(pd.read_csv('./2_raw_data/y-to-3rdDec.csv',\n",
    "                          index_col=0))\n",
    "y = y.append(pd.read_csv('./2_raw_data/y-to-19thDec.csv',\n",
    "                          index_col=0))                          \n",
    "y = y.sort_index()\n",
    "y.index = pd.to_datetime(y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_vars = ['d_price_NSW', 'd_price_QLD', 'd_price_SA', 'd_price_VIC',\n",
    "       'd_scheduledGeneration_NSW', 'd_scheduledGeneration_QLD',\n",
    "       'd_scheduledGeneration_SA', 'd_scheduledGeneration_VIC',\n",
    "       'scheduledGeneration_NSW', 'scheduledGeneration_QLD',\n",
    "       'scheduledGeneration_SA', 'scheduledGeneration_VIC', 'temperature_NSW',\n",
    "    'windSpeed_NSW', 'windDirection_NSW',\n",
    "       'temperature_QLD', 'windSpeed_QLD',\n",
    "       'windDirection_QLD', \n",
    "       'temperature_VIC', 'windSpeed_VIC', 'windDirection_VIC',\n",
    "       'temperature_SA',\n",
    "       'windSpeed_SA', 'windDirection_SA', 'cloudCover_SA', \n",
    "       'd_temperature_NSW',  'd_windSpeed_NSW',\n",
    "       'd_windDirection_NSW', \n",
    "       'd_temperature_QLD',  'd_windSpeed_QLD',\n",
    "       'd_windDirection_QLD',  'd_pressure_QLD',\n",
    "       'd_temperature_VIC',  'd_windSpeed_VIC',\n",
    "       'd_windDirection_VIC', \n",
    "       'd_temperature_SA',  'd_windSpeed_SA',\n",
    "       'd_windDirection_SA', \n",
    "       'price.Forecast_NSW', 'scheduledGeneration.Forecast_NSW',\n",
    "       'price.Forecast_QLD', 'scheduledGeneration.Forecast_QLD',\n",
    "       'price.Forecast_VIC', 'scheduledGeneration.Forecast_VIC',\n",
    "       'price.Forecast_SA', 'scheduledGeneration.Forecast_SA',\n",
    "       'd_price.Forecast_NSW', 'd_scheduledGeneration.Forecast_NSW',\n",
    "       'd_price.Forecast_QLD', 'd_scheduledGeneration.Forecast_QLD',\n",
    "       'd_price.Forecast_VIC', 'd_scheduledGeneration.Forecast_VIC',\n",
    "       'd_price.Forecast_SA', 'd_scheduledGeneration.Forecast_SA', \n",
    "       'dayofweek', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure equal shape on both X & Y, so both train and test data share the same timeframe\n",
    "df_all = df_all.interpolate(method='spline', \n",
    "                            order=3,\n",
    "                            limit_direction='forward', \n",
    "                            axis=0)\n",
    "df_all = df_all.dropna()\n",
    "y = y.dropna()\n",
    "\n",
    "# ensuring both are consistent:\n",
    "y = y[y.index.isin(df_all.index)]\n",
    "df_all = df_all[df_all.index.isin(y.index)]\n",
    "y = y[y.index.isin(df_all.index)]\n",
    "#y = y[(y.index>=df_all.index[0]) & (y.index<=df_all.index[-1])]\n",
    "assert all(dat in df_all.index for dat in y.index), 'Index differs for df_all and y!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_all.shape: ',df_all.shape)\n",
    "print('y.shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Linear, RFR, NN and CNN models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define data ranges for train, validation, and test set:\n",
    "\n",
    "# data the model will be trained on (corresponding columns)\n",
    "train_from = 0\n",
    "train_to = floor(0.9 * (df_all.shape[0] - 2))\n",
    "\n",
    "# data the current model (read: weights) are validated on\n",
    "valid_from = floor(0.9 * (df_all.shape[0] - 2))+1\n",
    "valid_to = floor(0.95 * (df_all.shape[0] - 2))\n",
    "\n",
    "# data the final fit will be tested on, to make predictions for the last timesteps\n",
    "test_from = floor(0.95 * (df_all.shape[0] - 2))+1\n",
    "test_to = df_all.shape[0] - 2\n",
    "\n",
    "# the last 24h will serve for inference purposes:\n",
    "inference_from = df_all.shape[0]-1\n",
    "nrow = df_all.shape[0]\n",
    "\n",
    "# create timestamps for the prediction df index:\n",
    "timeshifts = 24\n",
    "temp_index = pd.date_range(start=(df_all.index[inference_from-48] + \n",
    "                                  pd.Timedelta('1 hour')),\n",
    "                           freq='H',\n",
    "                           periods=timeshifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a multi linear model to the top i variables with the highest correlation to 'price_STATE':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a multi linear model to the top i variables with the highest correlation to 'price_SA':\n",
    "\n",
    "#df containing the predicted values with correct timestamps, and error list:\n",
    "lin_pred_df = pd.DataFrame(index=temp_index)\n",
    "mse_lin = pd.DataFrame(index=temp_index)\n",
    "\n",
    "for timeshift in range(1,13):\n",
    "    # Calculate correlations to price for slim and nimble multi-linear model\n",
    "    # account for timeseries nature of problem by cutting `timeshift` rows from\n",
    "    # X data, and shifting y by `timeshift` time steps:\n",
    "    # thus for all points in time: \n",
    "    # df_all.iloc[a:(b-timeshift),:] \n",
    "    # y.iloc[(a+timeshift):b,:]\n",
    "    corr_NSW   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,0])).corr()\n",
    "    corr_QLD   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,1])).corr()\n",
    "    corr_SA    =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,2])).corr()\n",
    "    corr_VIC   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,3])).corr()\n",
    "    state_dict =  {'NSW': corr_NSW, 'QLD': corr_QLD, 'VIC': corr_VIC, 'SA': corr_SA}\n",
    "\n",
    "    # fit for each state, calculate errors and make predictions:\n",
    "    for _, counter in zip(y.columns, range(len(y.columns))):\n",
    "        # extract state string:\n",
    "        state = _[_.index('_')+1:]\n",
    "\n",
    "        # test if result col already exists, if not create it:\n",
    "        temp_str = ''.join([state,'_linModel-inferred'])\n",
    "        if temp_str not in lin_pred_df.columns:\n",
    "            lin_pred_df[temp_str] = np.nan\n",
    "        if temp_str not in mse_lin.columns:\n",
    "            mse_lin[temp_str] = np.nan\n",
    "\n",
    "        # select vars with highest correlation to electricity spot price in state:\n",
    "        h_corr = list(state_dict[state][_].sort_values(ascending=False)[1:20].index)\n",
    "\n",
    "        # create training, validation and validation features:\n",
    "        # ignoring validation set here as only used in NN below\n",
    "        training_x = df_all[h_corr].iloc[train_from:(valid_to - timeshift),:]\n",
    "        training_y = y[[_]].iloc[timeshift:valid_to,:]\n",
    "        test_x = df_all[h_corr].iloc[test_from:(test_to - timeshift),:]\n",
    "        test_y = y[[_]].iloc[(test_from + timeshift):test_to,:]\n",
    "        \n",
    "        # get the (model, test-preds for mse) from the fit:\n",
    "        lin1, lin1_pred = tool.linear_model_predictions(training_x,                                             training_y,                                             test_x)\n",
    "\n",
    "        # mean squared error and relative rror on the validation data for efficacy test:\n",
    "        mse_lin.iloc[timeshift-1,counter] = tool.model_MSE(lin1_pred,                                                test_y)\n",
    "    \n",
    "        ## save model to folder ./5_models:\n",
    "        #now = datetime.now()\n",
    "        #dump(lin1, f'./5_models/{state}-lin-model-{now:y%Y-m%m-d%d-h%H-m%M}-error{mse:.1f}.joblib') # load model by using load('./folder/filename')\n",
    "\n",
    "        # save predictions to df:\n",
    "        infer = df_all[h_corr].iloc[inference_from,:]\n",
    "        lin_pred_df.iloc[timeshift-1,counter] = lin1.predict(infer.values.reshape((1,len(infer))))\n",
    "    \n",
    "    ## ...and also save preds to csv:\n",
    "    #lin_pred_df.to_csv('./6_predictions/lin-Model-predictions-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a random forest regressor model to the data for all 'price_STATE':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a Random Forest to the entire 60 variable dataset, testing for the influence of the train/test split:\n",
    "\n",
    "# create predictions df and errors df:\n",
    "RFR_pred_df = pd.DataFrame(index=temp_index)\n",
    "mse_RFR = pd.DataFrame(index=temp_index)\n",
    "\n",
    "start_time = time.time()\n",
    "for timeshift in range(1,25):\n",
    "    # Calculate correlations to price for slim and nimble model\n",
    "    # high corr and high feature importance variables have a high overlap, so\n",
    "    # as above high corr vars used for the forests\n",
    "    # account for timeseries nature of problem by cutting `timeshift` rows from\n",
    "    # X data, and shifting y by `timeshift` time steps:\n",
    "    # thus for all points in time: \n",
    "    # df_all.iloc[a:(b-timeshift),:] \n",
    "    # y.iloc[(a+timeshift):b,:]\n",
    "    corr_NSW   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,0])).corr()\n",
    "    corr_QLD   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,1])).corr()\n",
    "    corr_SA    =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,2])).corr()\n",
    "    corr_VIC   =  (df_all.iloc[:(nrow-timeshift),:].join(y.iloc[timeshift:,3])).corr()\n",
    "    state_dict =  {'NSW': corr_NSW, 'QLD': corr_QLD, 'VIC': corr_VIC, 'SA': corr_SA}\n",
    "\n",
    "    # fit for each state, calculate errors and make predictions:\n",
    "    for _, counter in zip(y.columns, range(len(y.columns))):\n",
    "        # extract state string:\n",
    "        state = _[_.index('_')+1:]\n",
    "\n",
    "        # test if result col already exists, if not create it:\n",
    "        temp_str = ''.join([state,'_RFRModel-inferred'])\n",
    "        if temp_str not in RFR_pred_df.columns:\n",
    "            RFR_pred_df[temp_str] = np.nan\n",
    "        if temp_str not in mse_RFR.columns:\n",
    "            mse_RFR[temp_str] = np.nan\n",
    "\n",
    "        # select vars with higher correlation to electricity spot price in state to improve preds:\n",
    "        h_corr = list(state_dict[state][_].sort_values(ascending=False)[1:22].index)    \n",
    "        # create train, test data (validation data is intrinsic due to oob):\n",
    "        # ignoring validation set here as only used in NN below\n",
    "        training_x = df_all[h_corr].iloc[train_from:(valid_to - timeshift),:]\n",
    "        training_y = y[[_]].iloc[timeshift:valid_to,:]\n",
    "        test_x = df_all[h_corr].iloc[test_from:(test_to - timeshift),:]\n",
    "        test_y = y[[_]].iloc[(test_from + timeshift):test_to,:]\n",
    "\n",
    "        RFR = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "        RFR.fit(training_x, np.ravel(training_y))\n",
    "        RFR_pred = RFR.predict(test_x)\n",
    "        \n",
    "        # mean square error and relative rror on the validation data for efficacy test:\n",
    "        mse_RFR.iloc[timeshift-1,counter] = tool.model_MSE(RFR_pred.reshape((len(RFR_pred),1)),test_y)\n",
    "\n",
    "        # save predictions to predictions dataframe:\n",
    "        infer = df_all[h_corr].iloc[inference_from,:]\n",
    "        RFR_pred_df.iloc[timeshift-1,counter] = RFR.predict(infer.values.reshape((1,len(infer))))\n",
    "        \n",
    "        ## save model to folder ./5_models:\n",
    "        # now = datetime.now()\n",
    "        # dump(RFR, f'./5_models/{state}-timeshift{timeshift}-RFR-model-{now:y%Y-m%m-d%d-h%H-m%M}.joblib') # load model by using load('./folder/filename')\n",
    "\n",
    "    # RFR_pred_df.to_csv('./6_predictions/RFR-Model-predictions-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)\n",
    "    \n",
    "print(\"--- {} seconds passed ---\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFR.feature_importances_\n",
    "# temp_df = pd.DataFrame()\n",
    "# temp_df['feature'] = [*temp_df.iloc[:,1:16].columns]#,'testfeature']\n",
    "# temp_df['importance'] = [*RFR.feature_importances_]\n",
    "# temp_df.sort_values(by=['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN/LSTM approach using multiple/single inputs, one output array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking by the '_XXX' substring\n",
    "# creates a list thereof, unpacks it (* unpacks), then adds others\n",
    "# unnamde cols:\n",
    "# df_NSW = df_all[[*df_all.columns[['_NSW' in a for a in df_all.columns]],'year', 'month', 'day', 'dayofweek','dayofyear', 'is_quarter_end', 'hour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recurrent neutral net using one input per state, returns array (len(array)=4)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import GRU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# some neural net hyperparameters (hp)\n",
    "batch_size = 72\n",
    "epochs = 26\n",
    "\n",
    "# GRU input layer\n",
    "# requiring input to be of dims (cols,1):\n",
    "dim = (df_all.shape[-1],1)\n",
    "input_GRU = Input(shape=(dim))\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} start time ---\")\n",
    "# Recurrent layer:\n",
    "GRU_layer = GRU(units=384,\n",
    "                activation='relu',\n",
    "                recurrent_dropout=0.4)(input_GRU)\n",
    "intermediate_layer = Dropout(0.2)(GRU_layer)\n",
    "output_layer = Dense(1)(intermediate_layer)\n",
    "\n",
    "model_RNN = Model(inputs=input_GRU,\n",
    "                outputs=output_layer, \n",
    "                name='GRU-unit')\n",
    "model_RNN.compile(loss='mse',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['mse'])\n",
    "RNN_1_pred_df = pd.DataFrame(columns=[*range(1,25)],\n",
    "                         index=y.index)\n",
    "mse_RNN = pd.DataFrame(columns=y.columns,\n",
    "                       index=temp_index)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for timeshift in range(1,25):#range(1,(timeshifts+1)):\n",
    "    test_y = y.iloc[(test_from+timeshift):test_to,2]\n",
    "    inputs = tool.reshape_for_CNN(df_all)\n",
    "\n",
    "    #     # This model optimises for the mse to be minimal testing model efficacy on\n",
    "    #     # validation data to check over-/underfitting:\n",
    "    history = model_RNN.fit(inputs[0:(train_to-timeshift),:],\n",
    "                  y.iloc[timeshift:train_to,2],\n",
    "                  validation_data=(inputs[valid_from:(valid_to-timeshift),:],\n",
    "                  y.iloc[(valid_from+timeshift):valid_to,2]),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=0)\n",
    "    \n",
    "    print(f'cycle {timeshift} training done!')\n",
    "    \n",
    "    # create predictions for mse calculation:\n",
    "    predictions = model_RNN.predict(inputs[test_from:(test_to-timeshift),:])\n",
    "\n",
    "    # save inference to predictions dataframe, need to be reshaped for right dims:\n",
    "    dim = inputs[(inference_from):].shape[0]\n",
    "    RNN_1_pred_df.iloc[:,timeshift-1] = model_RNN.predict(inputs)\n",
    "\n",
    "    # mean square error on the validation data for efficacy test:\n",
    "    mse_RNN.iloc[timeshift-1,:] = tool.MSE_CNN_2(predictions,test_y)[0]\n",
    "    \n",
    "    ## save model to folder ./5_models:\n",
    "    # now = datetime.now()\n",
    "    #model_CNN.save(f'./5_models3/RNN_model-epochs{epochs}-tshift{timeshift}.h5')\n",
    "    # load model by using: \n",
    "    # models.load_model('./5_models/CNN_1-model-y2019-m12-d17-h09-m20.h5')\n",
    "\n",
    "print(\"--- {} seconds passed ---\".format(time.time() - start_time))\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} end time ---\")\n",
    "# # save predictions:\n",
    "# # CNN_1_pred_df.to_csv(f'./6_predictions/CNN_1-Model-predictions-epochs{epochs}-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse: ',mse_RNN['price_SA'].mean(), ' rmse: ',np.sqrt(mse_RNN['price_SA'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model taking all inputs into consideration, optimising for each state individually, also factoring in time-like nature of problem in a causal manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, separate losses per state,\n",
    "inputs1 = df_all[cut_vars]\n",
    "model_CNN_2 = func_CNN_1.func_CNN_1(inputs1, nn_name='model_CNN_2')\n",
    "losses = {'NSW': 'mse',\n",
    "          'QLD': 'mse',\n",
    "          'VIC': 'mse',\n",
    "          'SA': 'mse'}\n",
    "metric = {'NSW': 'mse',\n",
    "          'QLD': 'mse',\n",
    "          'VIC': 'mse',\n",
    "          'SA': 'mse'}\n",
    "# compile model with the aboce losses, mse as it penalises larger errors more\n",
    "# optim = RMSprop(learning_rate=1.5E-4, rho=0.9)\n",
    "optim = Adam(learning_rate=2E-3, beta_1=0.9, beta_2=0.999)\n",
    "model_CNN_2.compile(loss=losses, optimizer=optim, metrics=metric)\n",
    "CNN_2_pred_df = pd.DataFrame(columns=[*range(1,25)], index=y.index)\n",
    "mse_CNN_2 = pd.DataFrame(columns=y.columns, index=temp_index)\n",
    "\n",
    "\n",
    "\n",
    "# define some hyper parameters:\n",
    "batch_sizes = 72\n",
    "epochs = 300\n",
    "\n",
    "# create the right tensor shape to feed into a convolutional-nn-layer:\n",
    "input_data = tool.reshape_for_CNN(df_all[cut_vars])\n",
    "\n",
    "# fit model to data:\n",
    "start_time = time.time()\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} start time ---\")\n",
    "\n",
    "for timeshift in range(1,25):#range(1,(timeshifts+1)):\n",
    "    t_train = {'NSW': y.iloc[(train_from + timeshift):train_to,0],\n",
    "        'QLD': y.iloc[(train_from + timeshift):train_to,1],\n",
    "        'SA': y.iloc[(train_from + timeshift):train_to,2],\n",
    "        'VIC': y.iloc[(train_from + timeshift):train_to,3]}\n",
    "    t_valid = {'NSW': y.iloc[(valid_from + timeshift):valid_to,0],\n",
    "               'QLD': y.iloc[(valid_from + timeshift):valid_to,1],\n",
    "               'SA': y.iloc[(valid_from + timeshift):valid_to,2],\n",
    "               'VIC': y.iloc[(valid_from + timeshift):valid_to,3]}\n",
    "    test_y = y.iloc[(test_from + timeshift):test_to,:]\n",
    "    \n",
    "    history_2 = model_CNN_2.fit(input_data[train_from:(train_to-timeshift),:],\n",
    "                t_train,\n",
    "                validation_data=(input_data[valid_from:(valid_to-timeshift),:],\n",
    "                t_valid),\n",
    "                batch_size=batch_sizes,\n",
    "                epochs=epochs,\n",
    "                shuffle=False,\n",
    "                verbose=0,\n",
    "                )#callbacks=[tensorboard_callback])\n",
    "    \n",
    "    now = datetime.now()\n",
    "    print(f'cycle {timeshift} training done at {now:%Y-%m-%d %H:%M}!')\n",
    "    # create predictions for test data: \n",
    "    in_ = input_data[test_from:(test_to-timeshift),:]\n",
    "    predictions_2 = np.array(model_CNN_2.predict(in_)).reshape(in_.shape[0],4)\n",
    "    \n",
    "    # save predictions to predictions dataframe, need to be reshaped for right dims:\n",
    "    # Arguably one could also go for .reshape(1,dim,max(timeshift)) here, for an\n",
    "    # automatic forcast of timeshift steps ahead in one go, but here it yielded\n",
    "    # quite, quite unreliable results...\n",
    "    # obvs would also require to structure input training data aboce in a corresponding\n",
    "    # manner, to arrange everythin in groups of (nrows,ncols,pred_length)\n",
    "    # e.g. df_all.values.reshape(#minibatches,ncols/dim,minibatch size)\n",
    "    # e.g. (3,2,2) would be [[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]]\n",
    "    temp = np.array(model_CNN_2.predict(input_data))\n",
    "    temp = temp.reshape(temp.shape[1],temp.shape[0])\n",
    "    CNN_2_pred_df.iloc[:,timeshift-1] = temp[:,2]\n",
    "    \n",
    "    # mean square error on the validation data for efficacy test:\n",
    "    mse_CNN_2.iloc[timeshift-1,:] = tool.MSE_CNN(predictions_2,test_y)[0]\n",
    "\n",
    "#     # save model to folder ./5_models:\n",
    "#     now = datetime.now()\n",
    "    model_CNN_2.save(f'./5_models2/CNN_2-model-t_shift{timeshift}-epochs{epochs}.h5')\n",
    "\n",
    "print(\"--- {} seconds passed ---\".format(time.time() - start_time))\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} end time ---\")\n",
    "# # load model by using: \n",
    "# # models.load_model('./5_models/CNN_2-model-y2019-m12-d17-h09-m20.h5')\n",
    "\n",
    "\n",
    "# CNN_2_pred_df.to_csv(f'./6_predictions/CNN_2-Model-predictions-epochs{epochs}-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse: ',mse_CNN_2['price_SA'].mean(), ' rmse: ',np.sqrt(mse_CNN_2['price_SA'].mean()))\n",
    "#CNN_2_pred_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performing CNN model taking a select number of all inputs optimising for SA only, also factoring in time-like nature of problem in a causal manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, separate losses per state,\n",
    "inputs1 = df_all[cut_vars]\n",
    "model_CNN_4 = func_CNN_1_inverted.func_CNN_1(inputs1, nn_name='model_CNN_2')\n",
    "losses = 'mse'\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "metric = [rmse]\n",
    "\n",
    "\n",
    "\n",
    "# compile model with the aboce losses, mse as it penalises larger errors more\n",
    "# optim = RMSprop(learning_rate=1.5E-4, rho=0.9)\n",
    "optim = Adam(learning_rate=2E-3)\n",
    "model_CNN_4.compile(loss=losses, optimizer=optim, metrics=metric)\n",
    "CNN_4_pred_df = pd.DataFrame(columns=[*range(1,25)], index=y.index)\n",
    "mse_CNN_4 = pd.DataFrame(columns=y.columns, index=temp_index)\n",
    "\n",
    "\n",
    "\n",
    "# define some hyper parameters:\n",
    "batch_sizes = 72\n",
    "epochs = 322\n",
    "\n",
    "# create the right tensor shape to feed into a convolutional-nn-layer:\n",
    "input_data = tool.reshape_for_CNN(df_all[cut_vars])\n",
    "\n",
    "# fit model to data:\n",
    "start_time = time.time()\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} start time ---\")\n",
    "\n",
    "for timeshift in range(1,25):#range(1,(timeshifts+1)):\n",
    "    t_train = y.iloc[(train_from + timeshift):train_to,2]\n",
    "    t_valid = y.iloc[(valid_from + timeshift):valid_to,2]\n",
    "    test_y =  y.iloc[(test_from + timeshift):test_to,2]\n",
    "    h_train = input_data[train_from:(train_to-timeshift),:]\n",
    "    h_valid = input_data[valid_from:(valid_to-timeshift),:]\n",
    "    \n",
    "    \n",
    "    history_4 = model_CNN_4.fit(h_train,\n",
    "                t_train,\n",
    "                validation_data=(h_valid,\n",
    "                                 t_valid),\n",
    "                batch_size=batch_sizes,\n",
    "                epochs=epochs,\n",
    "                shuffle=False,\n",
    "                verbose=0,\n",
    "                )#callbacks=[ES])\n",
    "    \n",
    "    now = datetime.now()\n",
    "    print(f'cycle {timeshift} done at {now:%Y-%m-%d %H:%M}!')\n",
    "    # create predictions for test data: \n",
    "    in_ = input_data[test_from:(test_to-timeshift),:]\n",
    "    predictions_4 = np.array(model_CNN_4.predict(in_)).reshape(in_.shape[0])\n",
    "    \n",
    "    # save predictions to predictions dataframe, need to be reshaped for right dims:\n",
    "    dim = input_data[inference_from].shape[0]\n",
    "    # Arguably one could also go for .reshape(1,dim,max(timeshift)) here, for an\n",
    "    # automatic forcast of timeshift steps ahead in one go, but here it yielded\n",
    "    # quite, quite unreliable results...\n",
    "    # obvs would also require to structure input training data aboce in a corresponding\n",
    "    # manner, to arrange everythin in groups of (nrows,ncols,pred_length)\n",
    "    # e.g. df_all.values.reshape(#minibatches,ncols/dim,minibatch size)\n",
    "    # e.g. (3,2,2) would be [[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]]\n",
    "    temp = np.array(model_CNN_4.predict(input_data))\n",
    "    temp = temp.reshape(temp.shape[0])\n",
    "    CNN_4_pred_df.iloc[:,timeshift-1] = temp\n",
    "    \n",
    "    # mean square error on the validation data for efficacy test:\n",
    "    mse_CNN_4.iloc[timeshift-1,2] = tool.MSE_CNN_df(predictions_4,test_y)\n",
    "\n",
    "#     # save model to folder ./5_models:\n",
    "#     now = datetime.now()\n",
    "#     model_CNN_2.save(f'./5_models/CNN_2-model-t_shift{timeshift}-epochs{epochs}.h5')\n",
    "\n",
    "print(\"--- {} seconds passed ---\".format(time.time() - start_time))\n",
    "print(f\"--- {time.asctime(time.localtime(time.time()))} end time ---\")\n",
    "# # load model by using: \n",
    "# # models.load_model('./5_models/CNN_2-model-y2019-m12-d17-h09-m20.h5')\n",
    "\n",
    "\n",
    "# CNN_2_pred_df.to_csv(f'./6_predictions/CNN_2-Model-predictions-epochs{epochs}-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse: ',mse_CNN_4['price_SA'].mean(), ' rmse: ',np.sqrt(mse_CNN_4['price_SA'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully connected NN model taking a select number of all inputs optimising for each state individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, separate losses per state,\n",
    "model_NN_3 = func_NN_3.func_NN_3(df_all[cut_vars], nn_name='model_NN_3')\n",
    "losses = {'NSW': 'mse',\n",
    "          'QLD': 'mse',\n",
    "          'VIC': 'mse',\n",
    "          'SA': 'mse'}\n",
    "metric = {'NSW': ['mse'],\n",
    "          'QLD': ['mse'],\n",
    "          'VIC': ['mse'],\n",
    "          'SA': ['mse']}          \n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=1.5E-4, rho=0.9)\n",
    "\n",
    "# compile model with the aboce losses, rmse as it penalises larger errors more\n",
    "model_NN_3.compile(loss=losses, optimizer=RMSprop, metrics=metric)\n",
    "NN_3_pred_df = pd.DataFrame(columns=[*range(1,25)], index=y.index)\n",
    "mse_NN_3 = pd.DataFrame(columns=y.columns, index=temp_index)\n",
    "\n",
    "# define some hyper parameters:\n",
    "batch_sizes = 96\n",
    "epochs = 23\n",
    "\n",
    "# fit model to data:\n",
    "start_time = time.time()\n",
    "\n",
    "for timeshift in range(1,25):\n",
    "    t_train = {'NSW': y.iloc[(train_from + timeshift):train_to,0],\n",
    "                 'QLD': y.iloc[(train_from + timeshift):train_to,1],\n",
    "                 'SA': y.iloc[(train_from + timeshift):train_to,2],\n",
    "                 'VIC': y.iloc[(train_from + timeshift):train_to,3]}\n",
    "    t_valid = {'NSW': y.iloc[(valid_from + timeshift):valid_to,0],\n",
    "                 'QLD': y.iloc[(valid_from + timeshift):valid_to,1],\n",
    "                 'SA': y.iloc[(valid_from + timeshift):valid_to,2],\n",
    "                 'VIC': y.iloc[(valid_from + timeshift):valid_to,3]}\n",
    "    test_y = y.iloc[(test_from + timeshift):test_to,:]\n",
    "\n",
    "    history_3 = model_NN_3.fit(df_all[cut_vars].iloc[train_from:(train_to - timeshift),:], \n",
    "                t_train,\n",
    "                validation_data=(df_all[cut_vars].iloc[valid_from:(valid_to - timeshift),:],\n",
    "                                 t_valid),\n",
    "                batch_size=batch_sizes,\n",
    "                epochs=epochs,\n",
    "                shuffle=False,\n",
    "                verbose=0)\n",
    "\n",
    "    # create predictions for test data: \n",
    "    in_ = df_all[cut_vars].iloc[test_from:(test_to-timeshift),:]\n",
    "    predictions_3 = np.array(model_NN_3.predict(in_))\n",
    "    predictions_3 = predictions_3.reshape(in_.shape[0],4)\n",
    "    \n",
    "    # save inference to inference dataframe, need to be reshaped for right dims:\n",
    "    in_2 =df_all[cut_vars]\n",
    "    temp = np.array(model_NN_3.predict(in_2)).reshape(df_all.shape[0],4)\n",
    "    NN_3_pred_df.iloc[:,timeshift-1] = temp[:,2]\n",
    "    \n",
    "    # mean square error on the validation data for efficacy test:\n",
    "    mse_NN_3.iloc[timeshift-1,:] = tool.MSE_CNN(predictions_3,\n",
    "                                                 test_y)[0]\n",
    "    \n",
    "\n",
    "print(\"--- {} seconds passed ---\".format(time.time() - start_time))\n",
    "\n",
    "    \n",
    "# # save model to folder ./5_models:\n",
    "# now = datetime.now()\n",
    "# model_NN_3.save(f'./5_models/NN_3-model-epochs{epochs}-{now:y%Y-m%m-d%d-h%H-m%M}.h5')\n",
    "# # load model by using: \n",
    "# # models.load_model('./5_models/NN_3-model-y2019-m12-d17-h09-m20.h5')\n",
    "\n",
    "\n",
    "\n",
    "# NN_3_pred_df.to_csv(f'./6_predictions/CNN_3-Model-predictions-epochs{epochs}-{now:y%Y-m%m-d%d-h%H-m%M}.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse: ',mse_NN_3['price_SA'].mean(), ' rmse: ',np.sqrt(mse_NN_3['price_SA'].mean()))\n",
    "#NN_3_pred_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some Plotting to check model performance over the course of training\n",
    "# plt.plot(history_2.history['val_SA_mean_squared_error'])\n",
    "# plt.plot(history_2.history['SA_mean_squared_error'])\n",
    "# plt.plot(history_2.history['SA_loss'])\n",
    "# plt.plot(history_2.history['val_SA_loss'])\n",
    "\n",
    "# plt.xlabel('EPOCHS')\n",
    "# plt.legend([f'val_SA_mean_squared_error',\n",
    "#             f'SA_mean_squared_error',\n",
    "#             f'SA_loss',\n",
    "#             f'val_SA_loss'],\n",
    "#              loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the mean abs errors, select smallest, use model to make inferences:\n",
    "mse_arr = [mse_lin['SA_linModel-inferred'].mean(), \n",
    "    mse_RFR['SA_RFRModel-inferred'].mean(), \n",
    "    mse_RNN['price_SA'].mean(),\n",
    "    mse_CNN_2['price_SA'].mean(), \n",
    "    mse_CNN_4['price_SA'].mean(),  \n",
    "    mse_NN_3['price_SA'].mean()]\n",
    "\n",
    "# get index of minimal member of list:\n",
    "index_min = min(range(len(mse_arr)), key=mse_arr.__getitem__)\n",
    "\n",
    "# get best predictions:\n",
    "pred_list = [lin_pred_df,\n",
    "             RFR_pred_df,\n",
    "             RNN_1_pred_df,\n",
    "             CNN_2_pred_df,\n",
    "             CNN_4_pred_df,\n",
    "             NN_3_pred_df]\n",
    "inference = pred_list[index_min].copy()\n",
    "inference.columns = [(_[_.index('_')+1:]).lower() for _ in y.columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mse_RNN['price_SA'].mean()),\n",
    "    np.sqrt(mse_CNN_2['price_SA'].mean()), \n",
    "    np.sqrt(mse_CNN_4['price_SA'].mean()),  \n",
    "    np.sqrt(mse_NN_3['price_SA'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload inferences to Amphora Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Upload data to Amphora Data:\n",
    "params = {'name': \"Electricity Spot Prices\",\n",
    "        'description': \"Trends for electricity spot prices. Absolute value may diverge from true value, but trend is valid.\",\n",
    "        'price': 0\n",
    "        }\n",
    " \n",
    "# upload        \n",
    "APIfetch.upload_series(inference, params, id_='3a2ad59f-4995-4375-87a4-c05da3537307')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create final plots:\n",
    "\n",
    "# import matplotlib.dates as mdates\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M')) #y%Y-m%m-d%d-h%H-m%M for 2019-12-11 13:42\n",
    "# plt.plot(y.price_SA[datetime(2019,11,25,11,tzinfo=timezone.utc):datetime(2019,11,29,11,tzinfo=timezone.utc)])\n",
    "# plt.plot(CNN_4.one[datetime(2019,11,25,11,tzinfo=timezone.utc):datetime(2019,11,29,11,tzinfo=timezone.utc)])\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(['True Prices', 'Predicted Prices'], loc='best')\n",
    "# plt.title('SA Price Spikes: Spot Price Troughs')\n",
    "# plt.ylabel('Predicted Prices (A$)')\n",
    "# plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create final plots:\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M')) #y%Y-m%m-d%d-h%H-m%M for 2019-12-11 13:42\n",
    "# plt.plot(y.price_SA[datetime(2019,12,17,11,tzinfo=timezone.utc):datetime(2019,12,19,9,tzinfo=timezone.utc)])\n",
    "# plt.plot(CNN_4.one[datetime(2019,12,17,11,tzinfo=timezone.utc):datetime(2019,12,19,9,tzinfo=timezone.utc)])\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(['True Prices', 'Predicted Prices'], loc='best')\n",
    "# plt.title('SA Price Spikes: Spot Price Peaks')\n",
    "# plt.ylabel('Predicted Prices (A$)')\n",
    "# plt.xlabel('Time')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7 Deep Learning",
   "language": "python",
   "name": "python37-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
